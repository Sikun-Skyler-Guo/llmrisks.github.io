<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Week 7: GANs and DeepFakes | Risks (and Benefits) of Generative AI and Large Language Models</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="https://llmrisks.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/fonts.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/finite.css">
    <link rel="shortcut icon" href="/images/uva.png">  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      


	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		


	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      
<div class="row" style="padding-top: 16pt;">
  <div class="column small-12 medium-10 medium-offset-1 end large-8 large-offset-0">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      
      <h1 itemprop="name">Week 7: GANs and DeepFakes</h1>
      <div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 October 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<p><author>Presenting Team: Aparna Kishore, Elena Long, Erzhen Hu, Jingping Wan </author></p>
<p><author>Blogging Team: Haochen Liu, Haolin Liu, Ji Hyun Kim, Stephanie Schoch, Xueren Ge </author></p>
<h1 id="monday-9-october-br-generative-adversarial-networks-and-deepfakes">Monday, 9 October: <br> Generative Adversarial Networks and DeepFakes</h1>
<div class="slide">
  <img src="../images/week7/A.JPG" alt="">
  <p>Today's topic is how to utilize generative adversarial networks to create fake images and how to identify the images generated by these models.</p>
</div>
<div class="slide">
  <img src="../images/week7/B.JPG" alt="">
  <p>Generative Adversarial Network (GAN) is a revolutionary deep learning framework that pits two neural networks against each other in a creative showdown. One network, the generator, strives to produce realistic data, such as images or text, while the other, the discriminator, aims to differentiate between genuine and generated data. Through a continuous feedback loop, GANs refine their abilities, leading to the generation of increasingly convincing and high-quality content.</p>
</div>
<div class="slide">
  <img src="../images/week7/C.JPG" alt="">
  <p>To ensure students have a better understanding of GANs. The leading team held a “GAN Auction Game” to simulate the generating and predicting process of the generator and discriminator in GAN. In this game, students are divided into two groups (Group 1 and Group 2). Group 1 will provide three items (e.g. the name of a place) while Group 2 tries to identify whether the items provided are real or fake.
<p>The game captures the training process of GANs where the generator first proposes certain contents (e.g. images or contexts) and the discriminator is trained to distinguish real from generated (fake) content.</p>
<p>If the generator successfully creates contents that fools the discriminator, it will receive a high reward for further tuning. On the other hand, if the discriminator correctly identifies the content created by the generator it receives a reward.</p>
<p>This iterative training process is illustrated by the figures below.</p></p>
</div>
<div class="slide">
  <img src="../images/week7/D.JPG" alt="">
</div>
<h2 id="heading"></h2>
<div class="slide">
  <img src="../images/week7/E.JPG" alt="">
</div>
<h2 id="heading-1"></h2>
<div class="slide">
  <img src="../images/week7/F.JPG" alt="">
</div>
<h2 id="heading-2"></h2>
<div class="slide">
  <img src="../images/week7/G.JPG" alt="">
  <p>Formally, the training process can be modeled as a two-player zero-sum game by conducting min-max optimization on the objective function.A Nash equilibrium will be established between generator and discriminator.</p>
</div>
<div class="slide">
  <img src="../images/week7/H.JPG" alt="">
</div>
<h1 id="heading-3"></h1>
<div class="slide">
  <img src="../images/week7/I.JPG" alt="">
  <p>
<p>For a system that only has generators and discriminators, it is hard to tell whether they are doing well because there are many bad local optima. Thus, one direct way is to introduce human feedback for evaluating.</p>
<p>For example, we can borrow strategies from Large Language Models (LLMs), particularly employing Reinforcement Learning from Human Feedback (RLHF). In this method, experts would iteratively rank the generated samples, offering direct reinforcement signals to improve the generator&rsquo;s output. This approach could enhance the realism and semantic alignment of the content created by GANs. However, the RLHF method has its drawbacks, primarily the extensive need for expert involvement, raising concerns about its scalability in larger evaluations.</p>
<p>An alternative could be the inclusion of non-expert users, offering a broader range of feedback. Crowdsourcing and user studies are suggested as methods to understand if the generated content meets the target audience&rsquo;s needs and preferences.</p></p>
<p>For images or tabular data, when the data distribution is roughly known, inception score serves as an useful metric. This score calculates the KL divergence between the conditional class distribution and the marginal class distribution of generated samples. A higher inception score (IS) indicates clearer and more diverse images. However, it doesn't always correlate with human judgment.</p>
</div>
<div class="slide">
  <img src="../images/week7/J.JPG" alt="">
<h2 id="heading-4"></h2>
<ol>
<li><p>Vanishing/Exploding Gradient: During backpropagation, gradients can shrink (vanish) or grow excessively (explode), disrupting learning. Vanishing gradients stall the network's learning, as parameter updates become negligible. Exploding gradients cause extreme, destabilizing updates, hindering the model's convergence.</p>
</li>
<li><p>Mode Collapse: GANs can suffer from mode collapse, where the generator produces limited, similar samples, failing to represent the data's true diversity. This occurs when the generator exploits the discriminator's weaknesses, concentrating on certain data aspects and neglecting others. It compromises the GAN's objective of generating diverse, realistic samples, indicating a breakdown in adversarial learning.</p>
</li>
</ol>
</div>
<div class="slide">
  <img src="../images/week7/K.JPG" alt="">
</div>
<h1 id="heading-5"></h1>
<div class="slide">
  <img src="../images/week7/L.JPG" alt="">
  <p>A warm-up game is to identify the fake person that is generated by GANs.</p>
</div>
<div class="slide">
  <img src="../images/week7/M.JPG" alt="">
  <p>In the above figure, one of the two faces is fake but it is difficult to identify at first glance.</p>
</div>
<div class="slide">
  <img src="../images/week7/N.JPG" alt="">
  <p>To successfully identify fake images, there are several methods that either use deep-learning-based models to learn to identify fake samples, or through direct observation by people. The leading team then introduces three interesting methods that enable us to tell the difference. We will revisit these two faces later and now focus on the detailed methods to do general identification.</p>
</div>
<div class="slide">
  <img src="../images/week7/O.JPG" alt="">
</div>
<h2 id="heading-6"></h2>
<div class="slide">
  <img src="../images/week7/P.JPG" alt="">
  <p>For example, images generated by GANs tend to contain color artifacts or invisible artifacts that can be identified by deep learning models.</p>
</div>
<div class="slide">
  <img src="../images/week7/Q.JPG" alt="">
  <p>The second method is physical-based. Namely, the corneal specular highlights for the real face have strong similarities while those for the GAN-faces are different.</p>
</div>
<div class="slide">
  <img src="../images/week7/R.JPG" alt="">
  <p>The third method is physiological-based. Specifically, the pupils for the real eyes have strong circular shapes while the GAN-generated pupils usually have irregular shapes.</p>
</div>
<div class="slide">
  <img src="../images/week7/S.JPG" alt="">
  <p>With the help of these methods, we can say that the left woman in the figure we showed before is fake. This can be justified by color artifacts of GAN-image identified from deep learning and her irregular pupils.</p>
</div>
<div class="slide">
  <img src="../images/week7/T.JPG" alt="">
  <p>The leading team also believes that these identification methods can be escaped by more advanced image-generating models but new methods will also be proposed accordingly to distinguish images generated by these advanced models. The generation and identification will evolve together.</p>
</div>
<div class="slide">
  <img src="../images/week7/U.JPG" alt="">
  <p>In summary, generative models such as GANs have fundamentally transformed people's lives, and there remains a substantial amount of future research and development ahead. Some future directions are listed above.</p>
</div>
<h1 id="wednesday-11-octoberbrcreation-and-detection-of-deepfake-videos">Wednesday, 11 October<br>Creation and Detection of DeepFake Videos</h1>
<div class="slide">
  <img src="../images/week7/1.png" alt="">
</div>
<h2 id="heading-7"></h2>
  <p> Outline
  <ol>
  <li>Introduction to deepfake videos</li>
  <li>Detecting Face-swap deepfakes with temporal dynamics</li>
  <li>Discussion</li>
  </ol>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/3.png" alt="">
<p> Definition of a deepfake: A deceptive image or recording that distorts reality to deceive.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/4.png" alt="">
</div>
<h1 id="heading-8"></h1>
<div class="slide">
  <img src="../images/week7/5.png" alt="">
  <p> There are some side effects of face swap methods, including
  <ul>
  <li> Limited accuracy </li>
  <li> Concerns of how to protect privacy </li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/6.png" alt="">
  <p> The presenters introduced three different methods of generating deepfake videos:
  <ol>
  <li> Reenactment </li>
  <li> Lip-sync deepfakes </li>
  <li> Text-based deepfake synthesis </li>
  </ol>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/7.png" alt="">
  <p> Reenactment: A deepfake reenacts using source images to manipulate the target.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/8.png" alt="">
  <p> Example of a reenactment: the mouth movement in Trump's video is animated by a source actor.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/9.png" alt="">
  <p>Here is another example of Reenactment, where the dancing in target video is animated by a source actor.</p>
</div>
<div class="slide">
  <img src="../images/week7/10.png" alt="">
  <p> Three main steps of reenactment:
  <ol>
  <li> The first step is tracking facial features in both source and target videos. </li>
  <li> A consistency measure aligns input video features with a 3D face model. </li>
  <li> Expressions are transferred from source to target with refinement for realism. </li>
  </ol>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/11.png" alt="">
  <p> Difference between face swap and reenactment: 
  <ul>
  <li> Difference is about the target image. Face swap retains part of the source image, but reenactment does not retain the background source image</li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/12.png" alt="">
  <p> Most methods use RGB images, while lip-sync relies on audio input.
  <ul>
  <li> Audio is transformed into a dynamic mouth shape. </li>
  <li> The mouth texture is matched with the target video for natural motion. </li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/13.png" alt="">
<p>Text-based methods modify videos per word, <strong>phonemes</strong> and <strong>visemes</strong> are key for pronunciation and analysis. Text edits are matched with phoneme sequences in the source video. Parameters of the 3D head model are used to smooth lip motions.</p>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/14.png" alt="">
  <p> While previous works have done a lot, an overlooked aspect in the creation of these deep-fake videos is the human ear. Here is one recent work trying to tackle this problem from the aspect of ear.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/15.png" alt="">
  <p> Three types of authentication techniques:
  <ul>
  <li> Forensic  Analysis </li>
  <li> Digital Signatures </li>
  <li>Digital Watermarks </li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/16.png" alt="">
  <p> Today, our focus is on forensic methods to detect deep fakes. These methods can be categorized into low- and high-level approaches.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/17.png" alt="">
  <p> The presenters asked the class to find criterias to identify an authentic picture of Tom Cruise. In the discussion, several factors were highlighted:
  <ul>
  <li> <i>Posture</i> of the third picture is unnatural.</li>
  <li><i>Foggy background</i> of first picture vs. <i>Realistic background</i> of the second picture</li>
  <li><i>Scale/ratio</i> of head and hand is odd in the third picture</li>
  </ul>
  During the class poll to determine which image appeared authentic, the majority of students voted for the second image, with a few supporting the first, and none voting for the third.
</p>
<p>
Surprisingly to the majority of the class, it was revealed that the first image was genuine, while the others were crafted by a TikTok user in creating deep fake content.
</p>
</div>
<div class="slide">
  <img src="../images/week7/18.png" alt="">
  <p> Many deep fake videos emphasizing facial expressions often neglect the intricate movements of the human ear and the corresponding changes that occur in jaw movements.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/19.png" alt="">
  <p> The aural dynamics system tracks and annotates ear landmarks, utilizing averaged local aural motion to simulate both horizontal and vertical movements, mirroring those of a real person.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/20.png" alt="">
  <p> With the videos of Joe Biden, Angela Merkel, Donald Trump, and Mark
Zuckerberg, they used GAN to synthesize the mouth region of individuals to match the new audio track and generate a lip-sync video.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/21.png" alt="">
  <p> The graphs are a distribution of the correlation of horizontal motion of three aural areas and audio (left) and lip vertical distance (right). </p>
  <p>Fake ones have no correlation, where individuals have strong correlation that are not necessarily consistent. </p>
</div>
<div class="slide">
  <img src="../images/week7/22.png" alt="">
  <p> The horizontal movement of the tragus and lobule parts of Trump’s ears exhibited a positive correlation, distinguishing it as a distinctive personal trait, unlike the general pattern observed on others.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/23.png" alt="">
  <p> The table shows the performance of each model. Models with person-specific training show a higher average testing accuracy.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/24.png" alt="">
  <p> Question 1: Limitations of the proposed methods & possible improvements
  <ul>
  <li> Group 1: Poor evaluation on high-quality videos, their training dataset is low-quality. </li>
  <li> Group 2: Ear detection is only possible when ear is visible. </li>
  <li> Group 3: Dependent on visibility of ear and having a reference image; ability to generalize to other situations, i.e.: smaller sample sizes; you could find an actor whose biometric markers were more similar to the desired image. </li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/25.png" alt="">
  <p> As mentioned, there are drawbacks such as when hair is hiding the movement of ears, large head movement, and accurate ear tracking is difficult. Still, more facial and audio signals can be further studied.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/26.png" alt="">
  <p> Question 2: Anomalies found from deep fake videos
  <ul>
  <li> Group 2: 
  <ul> 
  <li> <i>Shape of the mouth</i> is generally the same; it just expands and reduces in size to mimic mouth movements when speaking. Thus the bottom teeth is never shown. </li>
  <li><i>Light reflection on glasses</i> when reporters move their head is not generated. </li>
  <li><i>Lips not synced</i> properly (the word editing does not match).</li>
  </ul> 
  </li>
  <li> Group 3:
  <ul>
  <li> Fake video 1:
  <ul>
  <li>Lips seem constrained</li>
  <li>Eye blinking robotic, no change iin width</li>
  <li>Lack of nostril changes</li>
  </ul>
  </li>
  <li>Fake video 2:
  <ul>
  <li>Mouth/teeth</li>
  <li>Symmetry</li>
  </ul>
  </li>
  </ul>
  </li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/27.png" alt="">
  <p> The speaker of the first video does not blink for over 6 seconds, which is impossible. The average resting blinking rate should be 0.283 per second.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/28.png" alt="">
  <p> The second speaker’s lips are not closing for ‘m,’ ‘b,’ and ‘p’ (Phonemes-Visemes).
  </p>
</div>
<div class="slide">
  <img src="../images/week7/29.png" alt="">
  <p> Human pulse and respiratory motions are imperceptible to the human eye. Amplifying these factors could serve as a method for detecting generated videos.
  </p>
<p><strong>Note:</strong> The method was originally designed for medical purposes, aiming to identify potential health risks in a medical setting in a non-intrusive way. </p></p>
</div>
<div class="slide">
  <img src="../images/week7/30.png" alt="">
  <p> 
  <ul> 
  <li> <i>Group 1:</i> The “arm-race” is win-win development for both groups. Generation and detection will learn from the feedback of each side. </li>
  <li><i>Group 2:</i> There always will be a case where humans get creative and find out ways to improve and get away with detection, as fraud detection does. Optimally, if people don’t use it in an unethical way, it can be useful in various ways like the film industry.</li>
  <li><i>Group 3:</i> What if big companies become attackers (even for research purposes)?</li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/31.png" alt="">
<h1 id="heading-9"></h1>
  <p> Both the technology and ways to detect deep-fake videos will continue to advance.
However, it requires more than simply trying to generate and identify them.
By using watermarks, deep-fake videos can be distinguished from the source. Furthermore, public education on teaching importance on collecting information from the correct source and further government regulations can be considered. Perhaps the biggest threat from improvements in the quality and ease of creating fake imagery, is that people will lose confidence in all images and assume everything they see is fake.
  </p>
</div>
<h1 id="readings">Readings</h1>
<h3 id="for-the-first-class-109">For the first class (10/9)</h3>
<ul>
<li>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. <a href="https://arxiv.org/abs/1406.2661"><em>Generative adversarial nets</em></a>. 2014.</li>
<li>Xin Wang, Hui Guo, Shu Hu, Ming-Ching Chang, Siwei Lyu. <a href="https://arxiv.org/abs/2202.07145"><em>GAN-generated Faces Detection: A survey and new perspectives</em></a>. 2022.</li>
</ul>
<h3 id="for-the-second-class-1011">For the second class (10/11)</h3>
<ul>
<li>Shruti Agarwal and Hany Farid. <a href="https://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Agarwal_Detecting_Deep-Fake_Videos_From_Aural_and_Oral_Dynamics_CVPRW_2021_paper.html"><em>Detecting deep-fake videos from aural and oral dynamics</em></a>. CVPR 2023.</li>
</ul>
<h2 id="optional-additional-readings">Optional Additional Readings</h2>
<ul>
<li>Dilrukshi Gamage, Piyush Ghasiya, Vamshi Krishna Bonagiri, Mark E Whiting, and Kazutoshi Sasahara. <a href="https://dl.acm.org/doi/10.1145/3491102.3517446"><em>Are Deepfakes Concerning? Analyzing Conversations of Deepfakes on Reddit and Exploring Societal Implications</em></a>. In CHI Conference on Human Factors in Computing Systems (CHI ’22), April 29-May 5, 2022, New Orleans, LA, USA. ACM, New York, NY, USA, 19 pages.</li>
<li>Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, Richard G. Baraniuk. <a href="https://arxiv.org/abs/2307.01850"><em>Self-Consuming Generative Models Go MAD</em></a>.</li>
<li>Momina Masood, Marriam Nawaz, Khalid Mahmood Malik, Ali Javed, Aun Irtaza. <a href="https://arxiv.org/abs/2103.00484"><em>Deepfakes generation and detection: State-of-the-art, open challenges, countermeasures, and way forward</em></a>. Applied Intelligence, June 2022.</li>
</ul>
<h3 id="on-gan-training">On GAN Training</h3>
<ul>
<li>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., &amp; Chen, X. (2016). <a href="https://papers.nips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf"><em>Improved techniques for training gans</em></a></li>
<li>Arjovsky, M., &amp; Bottou, L. (2017). <a href="https://openreview.net/pdf?id=Hk4_qw5xe"><em>Towards principled methods for training generative adversarial networks</em></a></li>
<li>Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., &amp; Anderson, R. (2023). <a href="https://arxiv.org/abs/2305.17493"><em>The Curse of Recursion: Training on Generated Data Makes Models Forget</em></a></li>
</ul>
<h3 id="blogs-and-tutorials">Blogs and Tutorials</h3>
<ul>
<li><a href="https://medium.datadriveninvestor.com/how-generative-adversial-network-works-3ddce0062b9"><em>How Generative Adversarial Network Works</em></a></li>
<li><a href="https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf"><em>Increasing threat of deepfake identities</em></a></li>
</ul>
<h1 id="discussion-questions">Discussion Questions</h1>
<p><strong>For Monday&rsquo;s class:</strong> (as usual, post your response to at least one of these questions, or respond to someone else&rsquo;s response, or post anything you want that is interesting and relevant, before 8:29pm on Sunday, 8 October)</p>
<ol>
<li>How might the application of GANs extend beyond image generation to other domains, such as text, finance, healthcare (or any other domain that you can think of) and what unique challenges might arise in these different domains? How can the GAN framework ensure fairness, accountability, and transparency in these applications?</li>
<li>Considering the challenges in evaluating the performance and quality of GANs, how might evaluation metrics or methods be developed to assess the quality, diversity, and realism of the samples generated by GANs in a more robust and reliable manner? Additionally, how might these evaluation methods account for different types of data (e.g., images, text, tabular
etc.) and various application domains?</li>
<li>The authors identify 2 methods of detecting GAN-based images: physical and physiological. Is it possible that we can train a new model to modify a GAN-based image to hide these seemingly obvious flaws, like the reflection and pupil shapes? Will this approach quickly invalidate these two methods?</li>
<li>Do you agree with the authors that deep-learning based methods lack interpretability? Is the visible or invisible patterns detected by DL models really not understandable or explainable?</li>
</ol>
<p><strong>Questions for Wednesday&rsquo;s class:</strong> (post response by 8:29pm on Tuesday, 10 October)</p>
<ol>
<li>What are the potential applications for the techniques discussed in the Agarwal and Farid paper beyond deep-fake detection, such as in voice recognition or speaker authentication systems?</li>
<li>How robust are the proposed ear analysis methods to real-world conditions like different head poses, lighting, occlusion by hair?</li>
<li>What are your ideas for other ways to detect deepfakes?</li>
<li>Deepfake detection and generation seems similar to many other &ldquo;arms races&rdquo; between attackers and defenders. How do you see this arms race evolving? Will there be an endpoint with one side clearly winning?</li>
</ol>

      </div>

      <meta itemprop="wordCount" content="2490">
      <meta itemprop="datePublished" content="2023-10-16">
      <meta itemprop="url" content="https://llmrisks.github.io/week7/">
    </article>

    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="https://llmrisks.github.io/week5/">&laquo; <em>Previous<span class="show-for-sr"> page</span></em>: Week 5: Hallucination</a></li>
      
      
      <li class="arrow" aria-disabled="true"><a href="https://llmrisks.github.io/week8/"><em>Next<span class="show-for-sr"> page</span></em>: Week 8: Machine Translation&nbsp;&raquo;</a></li>
      
    </ul>

  </div>
</div>

    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-12 medium-6">
      <a href="//llmrisks.github.io"><b>cs 6501: Risks (and Benefits) of Generative AI</b></a><br>
      Fall 2023<br>
      <a href="//www.cs.virginia.edu">University of Virginia</a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="https://llmrisks.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
  </hr>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
